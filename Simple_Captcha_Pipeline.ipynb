{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunNarendra/455finalproject/blob/main/Simple_Captcha_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project: Breaking CAPTCHAs\n",
        "\n",
        "Our final project will involve solving CAPTCHAs, which are images with sequences of letters and numbers used to \"verify\" that a user is a human."
      ],
      "metadata": {
        "id": "FaeTRmYEaT3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Neural Network Modeling\n",
        "\n",
        "Since CAPTCHAs use uppercase, lowercase, and digits, we will be using the EMNIST dataset to create a classifier. After we train it, we can use the pre-trained weights. That means that this section of code should only be run once to train the classifier."
      ],
      "metadata": {
        "id": "HAB6IoYL_aSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def get_emnist_data():\n",
        "  # Need to perform a rotate and flip to properly orient the images\n",
        "  trainset = torchvision.datasets.EMNIST(root='./data', split='byclass', train=True, download=True,\n",
        "                                        transform=torchvision.transforms.Compose([\n",
        "                                            lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                                            lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                                            torchvision.transforms.ToTensor()\n",
        "                                        ]))\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=8)\n",
        "\n",
        "  testset = torchvision.datasets.EMNIST(root='./data', split='byclass', train=False, download=True,\n",
        "                                      transform=torchvision.transforms.Compose([\n",
        "                                          lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                                          lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                                          torchvision.transforms.ToTensor()\n",
        "                                      ]))\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=8)\n",
        "  # 0-9 are for digits, 10-35 are for uppercase letters, 36-61 are for lowercase letters\n",
        "  classes = []\n",
        "  for i in range(0, 10):\n",
        "    classes.append(i)\n",
        "  for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
        "    classes.append(letter)\n",
        "  for letter in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "    classes.append(letter)\n",
        "  return {'train': trainloader, 'test': testloader, 'classes': classes}"
      ],
      "metadata": {
        "id": "a0mfFPCl_leJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get EMNIST data\n",
        "data = get_emnist_data()"
      ],
      "metadata": {
        "id": "5QHAk4N1_yfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out details about the training data\n",
        "print(data['train'].__dict__)"
      ],
      "metadata": {
        "id": "EveXLnOB_2oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out details about the testing data\n",
        "print(data['test'].__dict__)"
      ],
      "metadata": {
        "id": "t8DNwVaT_3Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get images and labels for one batch of the training data\n",
        "dataiter = iter(data['train'])\n",
        "images, labels = next(dataiter)\n",
        "print(images.size())"
      ],
      "metadata": {
        "id": "yJole--N_5_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "def imshow(img):\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "# Show first batch of images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels for first 8 digits\n",
        "print(\"Labels:\" + ' '.join('%9s' % data['classes'][labels[j]] for j in range(8)))\n",
        "\n",
        "print(images.size())\n",
        "flat = torch.flatten(images, 1)\n",
        "print(flat.size())"
      ],
      "metadata": {
        "id": "X8gD31GCACaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Try to switch to the CPU if possible\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "gxSbGf7rADNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0005, verbose=1):\n",
        "  net.to(device)\n",
        "  losses = []\n",
        "  # We are using CrossEntropy loss and Stochastic Gradient Descent \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "  for epoch in range(epochs):\n",
        "    sum_loss = 0.0\n",
        "    for i, batch in enumerate(dataloader, 0):\n",
        "      # Get the inputs and associated labels for this particular batch of data\n",
        "      inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "      # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # Forward propogate, backward propogate, and update weights\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()  \n",
        "      optimizer.step()\n",
        "      # Print loss information every 100 batches\n",
        "      losses.append(loss.item())\n",
        "      sum_loss += loss.item()\n",
        "      if i % 100 == 99:\n",
        "        if verbose:\n",
        "          print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch + 1, i + 1, sum_loss / 100))\n",
        "        sum_loss = 0.0\n",
        "  return losses"
      ],
      "metadata": {
        "id": "7cCtbLxeAGjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Convolutional neural network with two convolutional layers\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    # Input 28x28x1 image\n",
        "    # 16 filters\n",
        "    # 3x3 filter size (they also have 3 channels)\n",
        "    # stride 2 (downsampling by factor of 2)\n",
        "    # Output image: 14x14x16\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
        "\n",
        "    # Input 14x14x16 image\n",
        "    # 32 filters\n",
        "    # 3x3x16 filter size (they also have 16 channels)\n",
        "    # stride 2 (downsampling by factor of 2)\n",
        "    # Output image: 7x7x32\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
        "\n",
        "    # Fully connected linear layer\n",
        "    self.fc1 = nn.Linear(1568, 62)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "OqQuvL5hANTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use simulated annealing for training ConvNet\n",
        "anneal_net = ConvNet()\n",
        "\n",
        "anneal_losses =  train(anneal_net, data['train'], epochs=2, lr=.1)\n",
        "anneal_losses += train(anneal_net, data['train'], epochs=2, lr=.01)\n",
        "anneal_losses += train(anneal_net, data['train'], epochs=2, lr=.001)\n",
        "\n",
        "plt.plot(anneal_losses)"
      ],
      "metadata": {
        "id": "lWEmmjRDAUWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Save the model weights into a file, then download that file\n",
        "torch.save(anneal_net.state_dict(), 'model_state.pth')\n",
        "files.download('model_state.pth')"
      ],
      "metadata": {
        "id": "w_IE53YTBiYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, dataloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader: \n",
        "      # Get images and labels for the current batch\n",
        "      images, labels = batch[0].to(device), batch[1].to(device)\n",
        "      # Get predicted labels from our network\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # Tally up the number of correct predictions that our network made\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return correct/total"
      ],
      "metadata": {
        "id": "qk74Y6HYAZS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AnnealNet train accuracy: %f\" % accuracy(anneal_net, data['train']))"
      ],
      "metadata": {
        "id": "hyzKxO6CAaCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AnnealNet test accuracy: %f\" % accuracy(anneal_net, data['test']))"
      ],
      "metadata": {
        "id": "LEIIr09LAgyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 2: Load images\n",
        "\n",
        "We load images of CAPTCHAs from a Kaggle dataset."
      ],
      "metadata": {
        "id": "-alIXVK5acuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: First, must upload kaggle.json with credentials\n",
        "# Install the kaggle library\n",
        "! pip install kaggle\n",
        "# Make a directory named .kaggle\n",
        "! mkdir ~/.kaggle\n",
        "# Copy the kaggle.json into this new directory\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "# Allocate the required permissions for this file\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "zbWxCYDYafo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the simple CAPTCHA dataset\n",
        "! kaggle datasets download fanbyprinciple/captcha-images"
      ],
      "metadata": {
        "id": "fpTtHl9UnsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "! unzip captcha-images.zip"
      ],
      "metadata": {
        "id": "p7m0FXOzpTuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torchvision\n",
        "import re\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Dictionary for images. Key is the filename information. Value is the pixel data\n",
        "# stored as a tensor.\n",
        "imagesDict = {}\n",
        "\n",
        "for im_path in glob.glob(\"captcha_images/*.png\"):\n",
        "  # Convert the image to one channel\n",
        "  oneChannelImage = Image.open(im_path).convert(\"L\")\n",
        "  # Transform the image to a numpy array\n",
        "  imageData = np.array(oneChannelImage, dtype=np.uint8)\n",
        "  # Extract relevant file name information\n",
        "  filename = re.search(r\"[A-Z1-9]{4}\", im_path).group()\n",
        "  # Add the key-value mapping to dictionary\n",
        "  imagesDict[filename] = imageData"
      ],
      "metadata": {
        "id": "auTK8kBj8_5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get all the tags of the images in this dataset\n",
        "imagesTags = list(imagesDict.keys())\n",
        "# Get the tag corresponding to the first image\n",
        "firstImageTag = imagesTags[0]\n",
        "# Get the image data corresponding to the tag\n",
        "imageData = imagesDict.get(firstImageTag)\n",
        "\n",
        "# Show this image with dimensions 24 x 72\n",
        "plt.gray()\n",
        "plt.imshow(imageData)\n",
        "plt.show()\n",
        "\n",
        "# Print out its dimensions\n",
        "print(imageData.shape)"
      ],
      "metadata": {
        "id": "lOhaAS9V-MIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Character Segmentation\n",
        "\n",
        "We find bounding boxes around each character in a CAPTCHA, extract the individual characters, and normalize them."
      ],
      "metadata": {
        "id": "63p7UH_NbRBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def segment_nonnoise(im):\n",
        "  # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ind_chars = []\n",
        "  MIN_CHAR_AREA = 50\n",
        "  to_segment = im \n",
        "  blurred = cv2.blur(to_segment, (5,5), 0)\n",
        "  # plt.gray()  \n",
        "  # plt.imshow(blurred)\n",
        "  # plt.show()\n",
        "  img_thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "  # plt.gray()  \n",
        "  # plt.imshow(img_thresh)\n",
        "  # plt.show()\n",
        "  # thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "  contours, hierarchy = cv2.findContours(img_thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "  # print(len(contours))\n",
        "  # for contour in contours:\n",
        "  #     if cv2.contourArea(contour) > MIN_CHAR_AREA:\n",
        "  #         [X, Y, W, H] = cv2.boundingRect(contour)\n",
        "  #         # TODO: uncomment and run the code with this line to generate some\n",
        "  #         # cool pics for the writeup\n",
        "  #         # cv2.rectangle(to_segment, (X, Y), (X + W, Y + H), (0,255, 0), 1)\n",
        "\n",
        "  # plt.gray()\n",
        "  # plt.imshow(to_segment)\n",
        "  # plt.show()\n",
        "\n",
        "  # cv2.boundingRect returns X, Y, and width and height of the bounding\n",
        "  # box. Use the box's X coordinate to sort the contours from left to right,\n",
        "  # which will make it easier to enumerate through individual characters\n",
        "  contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])\n",
        "  # print(len(contours))\n",
        "  for contour in contours:\n",
        "    # print(\"Contour!\")\n",
        "    if cv2.contourArea(contour) >= MIN_CHAR_AREA:\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "      char = im[y:y+h, x:x+w]\n",
        "\n",
        "      # EMNIST uses square images\n",
        "      square = max(w, h)\n",
        "\n",
        "      # Set background to white\n",
        "      square_char = np.zeros((square, square), dtype=np.uint8)\n",
        "      square_char[...] = 255\n",
        "\n",
        "      # center the image\n",
        "      x_off = (square - w) // 2\n",
        "      y_off = (square - h) // 2\n",
        "\n",
        "      square_char[y_off:y_off+h, x_off:x_off+w] = char\n",
        "      # plt.imshow(square_char)\n",
        "      # plt.show()\n",
        "      \n",
        "      # resize to 28x28 pixels, since this is the size EMNIST operates on.\n",
        "      # Use INTER_AREA since according to the docs, this is what works best\n",
        "      # for shrinking and for zooming it is approximately nearest-neighbors\n",
        "      # interpolation\n",
        "      adj_char = cv2.resize(square_char, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "      ind_chars.append(adj_char)\n",
        "\n",
        "      # plt.imshow(adj_char)\n",
        "      # plt.show()\n",
        "    \n",
        "  return ind_chars"
      ],
      "metadata": {
        "id": "KttntqurBQ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = segment_nonnoise(imageData)\n",
        "print(len(characters))\n",
        "for character in characters:\n",
        "  plt.imshow(character)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "NwozOvvcBWau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: CAPTCHA Processing\n",
        "\n",
        "Once we have trained our neural network model on the EMNIST dataset, we can proceed to test our model on a CAPTCHA. "
      ],
      "metadata": {
        "id": "ebA7cfgowW41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Convolutional neural network with two convolutional layers\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    # Input 28x28x1 image\n",
        "    # 16 filters\n",
        "    # 3x3 filter size (they also have 3 channels)\n",
        "    # stride 2 (downsampling by factor of 2)\n",
        "    # Output image: 14x14x16\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
        "\n",
        "    # Input 14x14x16 image\n",
        "    # 32 filters\n",
        "    # 3x3x16 filter size (they also have 16 channels)\n",
        "    # stride 2 (downsampling by factor of 2)\n",
        "    # Output image: 7x7x32\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
        "\n",
        "    # Fully connected linear layer\n",
        "    self.fc1 = nn.Linear(1568, 62)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "K44v7ypWDEXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Try to switch to the CPU if possible\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "LmHt2qnwDT9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Reload weights from previously trained model\n",
        "# NOTE: Must upload file that contains model's pre-trained weights\n",
        "anneal_net = ConvNet()\n",
        "anneal_net.to(device)\n",
        "state_dict = torch.load('model_state.pth')\n",
        "anneal_net.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "YRg5SXnDjmyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, dataloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader: \n",
        "      # Get images and labels for the current batch\n",
        "      images, labels = batch[0].to(device), batch[1].to(device)\n",
        "      # Get predicted labels from our network\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # Tally up the number of correct predictions that our network made\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return correct/total"
      ],
      "metadata": {
        "id": "SiqG-lMEDk0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def get_emnist_data():\n",
        "  # Need to perform a rotate and flip to properly orient the images\n",
        "  trainset = torchvision.datasets.EMNIST(root='./data', split='byclass', train=True, download=True,\n",
        "                                        transform=torchvision.transforms.Compose([\n",
        "                                            lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                                            lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                                            torchvision.transforms.ToTensor()\n",
        "                                        ]))\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=8)\n",
        "\n",
        "  testset = torchvision.datasets.EMNIST(root='./data', split='byclass', train=False, download=True,\n",
        "                                      transform=torchvision.transforms.Compose([\n",
        "                                          lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
        "                                          lambda img: torchvision.transforms.functional.hflip(img),\n",
        "                                          torchvision.transforms.ToTensor()\n",
        "                                      ]))\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=8)\n",
        "  # 0-9 are for digits, 10-35 are for uppercase letters, 36-61 are for lowercase letters\n",
        "  classes = []\n",
        "  for i in range(0, 10):\n",
        "    classes.append(i)\n",
        "  for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
        "    classes.append(letter)\n",
        "  for letter in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "    classes.append(letter)\n",
        "  return {'train': trainloader, 'test': testloader, 'classes': classes}"
      ],
      "metadata": {
        "id": "Lgo7bWq7DwI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_emnist_data()"
      ],
      "metadata": {
        "id": "Kx9XS8NhDw_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the pre-trained model gives reasonable accuracy on the EMNIST dataset\n",
        "print(\"AnnealNet train accuracy: %f\" % accuracy(anneal_net, data['train']))\n",
        "print(\"AnnealNet test accuracy: %f\" % accuracy(anneal_net, data['test']))"
      ],
      "metadata": {
        "id": "tFGAvBzBtu43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flipColor(character):\n",
        "  for i in range(character.shape[0]):\n",
        "    for j in range(character.shape[1]):\n",
        "      # Let's say that anything at or above 150 will go to 0\n",
        "      if character[i, j] > 150:\n",
        "        character[i, j] = 0\n",
        "      else:\n",
        "        character[i, j] = 255\n",
        "  return character"
      ],
      "metadata": {
        "id": "9aUL80wOFVQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createIndexToCharacterMap():\n",
        "  indexToCharacter = []\n",
        "  for i in range(0, 10):\n",
        "    indexToCharacter.append(i)\n",
        "  for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
        "    indexToCharacter.append(letter)\n",
        "  for letter in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "    indexToCharacter.append(letter)\n",
        "  return indexToCharacter"
      ],
      "metadata": {
        "id": "HcA2f7rwI7E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "indexToCharacterMap = createIndexToCharacterMap()\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Feed each character of the CAPTCHA into the neural network and make a prediction\n",
        "for character in characters:\n",
        "  # Flip color schema of character to match that of the EMNIST dataset\n",
        "  character = flipColor(character)\n",
        "  plt.imshow(character)\n",
        "  plt.show()\n",
        "  # Transform ndarray to tensor before feeding into character into neural network\n",
        "  character = transform(character)\n",
        "  with torch.no_grad():\n",
        "    output = anneal_net(character.unsqueeze(0).to(device))\n",
        "    modelPrediction = output.argmax(dim=1)\n",
        "    print(\"Prediction: \" + str(indexToCharacterMap[modelPrediction.item()]))\n"
      ],
      "metadata": {
        "id": "P1PDPof_XNj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Putting it all Together\n",
        "\n",
        "Let us see how well our model performs on all the CAPTCHA images in the dataset."
      ],
      "metadata": {
        "id": "3Px5GRRfLg3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "def accuracy(net, imagesDict):\n",
        "  # Initial set up\n",
        "  numCorrectCAPTCHAs = 0\n",
        "  numTotalCAPTCHAs = 0\n",
        "  numCorrectCharacters = 0\n",
        "  numTotalCharacters = 0\n",
        "  indexToCharacterMap = createIndexToCharacterMap()\n",
        "  transform = transforms.Compose([transforms.ToTensor()]) \n",
        "\n",
        "  # Go through each CAPTCHA image in the dataset and calculate relevant statistics\n",
        "  for imageTag in imagesDict:\n",
        "    imageData = imagesDict.get(imageTag)\n",
        "    characters = segment_nonnoise(imageData)\n",
        "    numCorrectCharactersInCAPTCHA = 0\n",
        "    currCharacter = 0\n",
        "    for character in characters:\n",
        "      character = flipColor(character)\n",
        "      character = transform(character)\n",
        "      with torch.no_grad():\n",
        "        output = anneal_net(character.unsqueeze(0).to(device))\n",
        "        modelPrediction = output.argmax(dim=1)\n",
        "        prediction = str(indexToCharacterMap[modelPrediction.item()])\n",
        "        if currCharacter <= 3 and prediction == imageTag[currCharacter]:\n",
        "          numCorrectCharactersInCAPTCHA = numCorrectCharactersInCAPTCHA + 1\n",
        "        currCharacter = currCharacter + 1\n",
        "\n",
        "    # Perform updates\n",
        "    if numCorrectCharactersInCAPTCHA == 4:\n",
        "      numCorrectCAPTCHAs = numCorrectCAPTCHAs + 1\n",
        "    numCorrectCharacters = numCorrectCharacters + numCorrectCharactersInCAPTCHA\n",
        "    numTotalCAPTCHAs = numTotalCAPTCHAs + 1\n",
        "    numTotalCharacters = numTotalCharacters + 4\n",
        "    \n",
        "  return [numCorrectCAPTCHAs / numTotalCAPTCHAs, numCorrectCharacters / numTotalCharacters]"
      ],
      "metadata": {
        "id": "m2K_1mRSMgmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = accuracy(anneal_net, imagesDict)\n",
        "print(\"CAPTCHA accuracy: %f\" % accuracies[0])\n",
        "print(\"Character accuracy: %f\" % accuracies[1])"
      ],
      "metadata": {
        "id": "EUvTI_hJRHn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}